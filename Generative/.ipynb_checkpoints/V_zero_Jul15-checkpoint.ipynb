{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "202e6678-5220-4115-8a54-39466e07a2ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e229aa2-fe67-4338-a89a-8ce42ed695bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(SEED=1234):\n",
    "   random.seed(SEED)\n",
    "   np.random.seed(SEED)\n",
    "   torch.manual_seed(SEED)\n",
    "   torch.cuda.manual_seed(SEED)\n",
    "   torch.cuda.manual_seed_all(SEED)\n",
    "   torch.backends.cudnn.enabled = True\n",
    "   torch.backends.cudnn.benchmark = False\n",
    "   torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_random_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90d81488-5b7b-47ae-8979-c23ada496510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max absolute difference: tensor(4.0000)\n",
      "Are tensors close? False\n"
     ]
    }
   ],
   "source": [
    "def improved_soft_score(x, x_samples, t, sigma, M):\n",
    "    batch_size, channels, height, width = x.shape\n",
    "    num_samples = x_samples.shape[0]\n",
    "    x_samples_flat = x_samples.view(num_samples, -1)  # Shape: (num_samples, 784)\n",
    "    \n",
    "    # Initialize softmax accumulator\n",
    "    softmax_acc = torch.zeros((num_samples, batch_size), device=x.device)\n",
    "    \n",
    "    for i in range(M): \n",
    "        if sigma > 0:\n",
    "            x1 = x + torch.randn_like(x) * sigma\n",
    "        else:\n",
    "            x1 = x  # No cloning when sigma is 0\n",
    "        x_flat = x1.view(batch_size, -1)  # Shape: (batch_size, 784)\n",
    "    \n",
    "        # Compute differences: (num_samples, batch_size, 784)\n",
    "        diff = x_samples_flat.unsqueeze(1) - x_flat.unsqueeze(0)\n",
    "        \n",
    "        # Compute squared differences and x_samples squared\n",
    "        diff_squared = (diff ** 2).sum(dim=-1)  # Shape: (num_samples, batch_size)\n",
    "        x_samples_squared = (x_samples_flat ** 2).sum(dim=-1, keepdim=True)  # Shape: (num_samples, 1)\n",
    "        \n",
    "        # Compute log weights with numerical stability\n",
    "        log_weights = -diff_squared / (2 * (1 - t)) + x_samples_squared / 2\n",
    "        log_weights_max = torch.max(log_weights, dim=0, keepdim=True)[0]\n",
    "        \n",
    "        weights = torch.exp(log_weights - log_weights_max)\n",
    "        normalization = weights.sum(dim=0, keepdim=True)\n",
    "        softmax = weights / normalization\n",
    "        \n",
    "        softmax_acc += softmax\n",
    "    \n",
    "    softmax_avg = softmax_acc / M\n",
    "    \n",
    "    # Compute weighted sum of differences\n",
    "    weighted_diff = (diff * softmax_avg.unsqueeze(-1)).sum(dim=0)  # Shape: (batch_size, 784)\n",
    "    \n",
    "    # Compute the score estimate\n",
    "    score_estimate = weighted_diff / (1 - t)\n",
    "    \n",
    "    return score_estimate.view(batch_size, channels, height, width)\n",
    "\n",
    "# Test the equivalence again\n",
    "hard_result = hard_score(x, x_samples, t)\n",
    "improved_soft_result = improved_soft_score(x, x_samples, t, sigma, M)\n",
    "\n",
    "print(\"Max absolute difference:\", torch.max(torch.abs(hard_result - improved_soft_result)))\n",
    "print(\"Are tensors close?\", torch.allclose(hard_result, improved_soft_result, rtol=1e-5, atol=1e-8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e396f6-9c8c-4cf7-9ab0-4ceac811b675",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48a6471e-d07e-4750-be74-8864099f67e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_dataset():\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def hard_score(x, x_samples, t):\n",
    "    batch_size, channels, height, width = x.shape\n",
    "    num_samples = x_samples.shape[0]\n",
    "    \n",
    "    x_flat = x.view(batch_size, -1)  # Shape: (32, 784)\n",
    "    x_samples_flat = x_samples.view(num_samples, -1)  # Shape: (60000, 784)\n",
    "    \n",
    "    # Compute differences: (1, 32, 784) - (60000, 1, 784)\n",
    "    diff = x_flat.unsqueeze(0) - x_samples_flat.unsqueeze(1)\n",
    "    \n",
    "    # Compute squared differences and x_samples squared\n",
    "    diff_squared = (diff ** 2).sum(dim=-1)  # Shape: (60000, 32)\n",
    "    x_samples_squared = (x_samples_flat ** 2).sum(dim=-1, keepdim=True)  # Shape: (60000, 1)\n",
    "    \n",
    "    # Compute log weights with numerical stability\n",
    "    log_weights = -diff_squared / (2 * (1 - t)) + x_samples_squared / (2)\n",
    "    log_weights_max = torch.max(log_weights, dim=0, keepdim=True)[0]\n",
    "    \n",
    "    weights = torch.exp(log_weights - log_weights_max)\n",
    "    normalization = weights.sum(dim=0, keepdim=True)\n",
    "    softmax =  weights / normalization\n",
    "    \n",
    "    weighted_diff = (diff * softmax.unsqueeze(-1)).sum(dim=0)  # Shape: (batch_size, 784)\n",
    "    \n",
    "    # Compute the score estimate\n",
    "    score_estimate = weighted_diff / (1 - t)\n",
    "\n",
    "    return score_estimate.view(batch_size, channels, height, width)\n",
    "\n",
    "\n",
    "\n",
    "def soft_score(x, x_samples, t, sigma, M):\n",
    "    batch_size, channels, height, width = x.shape\n",
    "    num_samples = x_samples.shape[0]\n",
    "    x_samples_flat = x_samples.view(num_samples, -1)  # Shape: (60000, 784)\n",
    "    \n",
    "    # Initialize softmax accumulator\n",
    "    softmax_acc = torch.zeros((num_samples, batch_size), device=x.device)\n",
    "    \n",
    "    for i in range(M): \n",
    "        x1 = x.clone()\n",
    "        z = torch.randn_like(x1) * sigma\n",
    "        x1 += z\n",
    "        x_flat = x1.view(batch_size, -1)  # Shape: (batch_size, 784)\n",
    "    \n",
    "        # Compute differences: (num_samples, batch_size, 784)\n",
    "        diff = x_samples_flat.unsqueeze(1) - x_flat.unsqueeze(0)\n",
    "        \n",
    "        # Compute squared differences and x_samples squared\n",
    "        diff_squared = (diff ** 2).sum(dim=-1)  # Shape: (num_samples, batch_size)\n",
    "        x_samples_squared = (x_samples_flat ** 2).sum(dim=-1, keepdim=True)  # Shape: (num_samples, 1)\n",
    "        \n",
    "        # Compute log weights with numerical stability\n",
    "        log_weights = -diff_squared / (2 * (1 - t)) + x_samples_squared / 2\n",
    "        log_weights_max = torch.max(log_weights, dim=0, keepdim=True)[0]\n",
    "        \n",
    "        weights = torch.exp(log_weights - log_weights_max)\n",
    "        normalization = weights.sum(dim=0, keepdim=True)\n",
    "        softmax = weights / normalization\n",
    "        \n",
    "        softmax_acc += softmax\n",
    "    \n",
    "    softmax_avg = softmax_acc / M\n",
    "    \n",
    "    # Compute weighted sum of differences\n",
    "    weighted_diff = (diff * softmax_avg.unsqueeze(-1)).sum(dim=0)  # Shape: (batch_size, 784)\n",
    "    \n",
    "    # Compute the score estimate\n",
    "    score_estimate = weighted_diff / (1 - t)\n",
    "    \n",
    "    return score_estimate.view(batch_size, channels, height, width)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "071b0cbd-91d7-438c-8fd5-b4aabe78e2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score_hard estimate min: -1.953590989112854, max: 2.000000476837158\n",
      "score_soft estimate min: -2.000000476837158, max: 1.953590989112854\n"
     ]
    }
   ],
   "source": [
    "# Load entire MNIST dataset\n",
    "mnist_dataset = load_mnist_dataset()\n",
    "x_samples = torch.stack([img for img, _ in mnist_dataset]).to(device)\n",
    "\n",
    "# Generate a batch of images to compute the score for\n",
    "batch_size = 4\n",
    "dataloader = DataLoader(mnist_dataset, batch_size=batch_size, shuffle=True)\n",
    "x = next(iter(dataloader))[0].to(device)\n",
    "x = torch.zeros(x.shape, device=device)\n",
    "\n",
    "\n",
    "# Set the time parameter t\n",
    "t = 0.5  # You can adjust this value between 0 and 1\n",
    "sigma = 0\n",
    "M = 1\n",
    "\n",
    "# Compute the score estimate\n",
    "hardscore = hard_score(x, x_samples, t)\n",
    "softscore = soft_score(x, x_samples, t, sigma, M)\n",
    "\n",
    "\n",
    "print(f\"score_hard estimate min: {hardscore.min().item()}, max: {hardscore.max().item()}\")\n",
    "print(f\"score_soft estimate min: {softscore.min().item()}, max: {softscore.max().item()}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0361ad8f-b962-4f4c-b3d5-b72cc483e885",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-3\n",
    "num_steps = 400\n",
    "batch_size, chan, w, h = x.shape\n",
    "batch_size = 1\n",
    "t = torch.linspace(0, 1-eps, num_steps, device=device)\n",
    "delta_t = t[1] - t[0]\n",
    "x = torch.zeros(batch_size, chan, w, h, device= device)\n",
    "#s = torch.zeros(num_steps, batch_size, chan, w, h, device= device)\n",
    "\n",
    "sigma = 0\n",
    "M = 1\n",
    "\n",
    "i = 0\n",
    "for tt in t:\n",
    "    #score_estimate = hard_score(x, x_samples, tt)\n",
    "    score_estimate = soft_score(x, x_samples, tt, sigma, M)\n",
    "\n",
    "    #print(f\"Any NaN values: {torch.isnan(score_estimate).any().item()}\")\n",
    "    #print(f\"Any Inf values: {torch.isinf(score_estimate).any().item()}\")\n",
    "\n",
    "    x  = x - score_estimate * delta_t + torch.randn_like(x, device=device) * torch.sqrt(delta_t)\n",
    "    #s[i, :, :, :, :] = x\n",
    "    i += 1\n",
    "    #print(tt.item(), end='\\t')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8847239e-e258-40ba-ad4b-84d44ce23f60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_images(images):\n",
    "    images =  (images * 127.5 + 128).clip(0, 255).to(torch.uint8)\n",
    "    sample_grid = make_grid(images, nrow=int(np.sqrt(images.shape[0])))\n",
    "    #plt.figure(figsize=(6,6))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(sample_grid.cpu().permute(1, 2, 0).squeeze())\n",
    "    plt.show()\n",
    "\n",
    "y = torch.ones(x.shape, device =device)\n",
    "#plot_images(Y[0:64, :, :, :])\n",
    "plot_images(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8579cde9-8760-43ec-9ba3-e6c3dba2e38e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_samples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m size, _, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mx_samples\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m      2\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(size):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_samples' is not defined"
     ]
    }
   ],
   "source": [
    "size, _, _, _ = x_samples.shape\n",
    "k = 3\n",
    "for i in range(size):\n",
    "    a = torch.sum(x_samples[i, :, :, :] * x[k, :, :, :]) \n",
    "    b = torch.sum(x_samples[i, :, :, :] * x_samples[i, :, :, :]) * torch.sum(x[k, :, :, :] * x[k, :, :, :])\n",
    "    corr = a / torch.sqrt(b) \n",
    "    if corr > 0.99:\n",
    "       print(corr.item())\n",
    "       plot_images(x_samples[i, :, :, :])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bd3c81-6770-4c28-8afb-baef5c131092",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
